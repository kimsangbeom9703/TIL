# [Crawling] 파이썬 네이버 검색 뉴스 메뉴 크롤링하기

## 수집 내용
```
    네이버 검색결과 중 뉴스 검색결과 수집.
```

## 소스
```python
    import csv # csv 파일 저장
    import requests # req 요청
    import time # sleep
    from bs4 import BeautifulSoup # 페이지 파싱
    from itertools import count  # 반복데이터 처리
    from collections import OrderedDict # 데이터 처리

    URL = 'https://search.naver.com/search.naver' # 네이버 검색 주소
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36",
        'referer': 'https://naver.com'
    }
    # userAgent : 사용자 정보 / referer : 어디서 왔는지?

    postDict = OrderedDict()
    cnt = 1
    dcnt = 0
    query = '파이썬 공부'
    csvFile = open("./files/"+query+".csv", 'wt+', encoding='utf-8-sig', newline='')
    try:
        writer = csv.writer(csvFile)
        for page in count(1, 1):
            print((page - 1) * 10 + 1)
            param = {
                'where': 'news',
                'query': query,
                'start': (page - 1) * 10 + 1 # 네이버 페이징이 1 11 21 31 41 이런식으로 되어있다.
            }
            response = requests.get(URL, params=param, headers=headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            links = soup.select(".news_tit")  # 리스트 형태
            postDict = links
            if links:
                for link in links:
                    title = link.text  # 태그 안에 텍스트 요소
                    url = link.attrs['href']  # href의 속성 값
                    print((title, url))
                    writer.writerow((title, url)) # 파일 작성
                if links in postDict: # 마지막 전체 배열이 중복이면 종료
                    exit()
            else:
                exit()
            time.sleep(5) # 5초의 텀을 줌 ( 너무 빠를 경우 웹페이지가 열리기 전에 끝나기 때문에 시간을 줘야함.)
    finally:
        csvFile.close()
```

## 특이사항
```
    페이지가 몇 페이지까지 있는지 알 수 없어서 반복하다가 마지막 데이터를 딕셔너리에 삽입하고 마지막 데이터와 같으면 종료한다.
    sleep을 주는 이유는 너무 빠를 경우 웹페이지가 열리고 사이트가 로딩되기 전에 소스가 끝날 수도 있고 크롤링하는 곳에서 차단을 피하기 위함이다.
    또 특정 시간마다 들어올 경우 차단을 당할 수 있어서 무작위로 시간을 주는 것도 좋다.
```